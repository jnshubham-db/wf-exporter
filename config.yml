initial_variables:
  v_start_path: /Workspace/Applications/wf_exporter/exports/
  v_resource_key_job_id_mapping_csv_file_path: '{v_start_path}/bind_scripts/resource_key_job_id_mapping.csv'
  v_backup_jobs_yaml_path: '{v_start_path}/backup_jobs_yaml/'
  v_log_level: INFO
  v_databricks_yml_path: ./databricks.yml
  v_log_directory_path: '{v_start_path}/logs'

spark_conf_key_replacements:
- search_key: spark.hadoop.fs.azure.account.key.storage.dfs.core.windows.net
  target_key: spark.sql.shuffle.partitions
  target_value: '{existing_value}\nspark.hadoop.fs.azure.account.key.${var.v_storage_account}.dfs.core.windows.net
    ${var.v_storage_account_secret}'

path_replacement:
  ^/Workspace/Repos/[^/]+/: ../
  ^/Repos/[^/]+/: ../
  ^/Workspace/: ../
  ^/Shared/: ../
  ^/: ../

global_settings:
  export_libraries: true

value_replacements:
  ${: $${

workflows:
  - job_name: "Sample Workflow"
    job_id: 123456789
    is_existing: true
    is_active: true
    export_libraries: true

pipelines:
  - pipeline_name: "Sample Pipeline"
    pipeline_id: 987654321
    is_existing: true
    is_active: true
    export_libraries: true
