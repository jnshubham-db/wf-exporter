# WF Exporter

Export Databricks workflows and DLT pipelines as YAML files with support for both local development environments and Databricks notebook environments.

## üöÄ Features

- **Automatic Environment Detection** - Works in both local and Databricks environments
- **Multiple Authentication Methods** - Profile-based, environment variables, or auto-detection
- **Dual Asset Type Support** - Process both workflows and pipelines in a single run
- **Library Export Control** - Granular control over library artifact downloads
- **Modular Architecture** - Clean, maintainable codebase with separated concerns
- **Configurable Paths** - Customize output directories and log locations
- **YAML Processing** - Advanced transformations and replacements
- **Bundle Generation** - Creates Databricks Asset Bundle compatible files

## üìã Prerequisites

### Required Files

1. **`config/config.yml`** - Main configuration file
2. **`config/databricks.yml`** - Databricks bundle configuration file

### System Requirements

- Python 3.11+
- Databricks CLI (auto-installed in Databricks environments)
- Access to Databricks workspace

## üì• Installation

### Download from GitHub Releases

```bash
# Get the latest release wheel file
curl -L -o wfexporter-1.0.6-py3-none-any.whl \
  "https://github.com/YOUR_ORG/YOUR_REPO/releases/download/v1.0.6/wfexporter-1.0.6-py3-none-any.whl"

# Install the wheel
pip install wfexporter-1.0.6-py3-none-any.whl
```

### Alternative: Clone and Install

```bash
git clone <repository-url>
cd wf-exporter
poetry install
```

## üéØ Usage

### Command Line Interface

```bash
# Basic usage with config file
wf-export --config config/config.yml

# With explicit credentials
wf-export --config config/config.yml --host <databricks-host> --token <databricks-token>

# Help
wf-export --help
```

### Databricks Workflow Integration

1. **Upload wheel file** to Databricks workspace
2. **Create a job** with the following configuration:

```python
# Install the wheel in your job
%pip install /path/to/wfexporter-1.0.6-py3-none-any.whl

# Use in your notebook
from wfExporter import main
main('config/config.yml')
```

### Programmatic Usage

```python
from wfExporter import main, DatabricksExporter

# Simple usage
main('config/config.yml')

# Advanced usage
exporter = DatabricksExporter('config/config.yml')
exporter.run()
```

## ‚öôÔ∏è Configuration

### config.yml Structure

```yaml
initial_variables:
  # Output directory for generated files
  v_start_path: "/path/to/exports"
  
  # CSV mapping file path
  v_resource_key_job_id_mapping_csv_file_path: "{v_start_path}/bind_scripts/resource_key_job_id_mapping.csv"
  
  # Backup directory for YAML files
  v_backup_jobs_yaml_path: "{v_start_path}/backup_jobs_yaml/"
  
  # Log directory (configurable)
  v_log_directory_path: "{v_start_path}/logs"
  
  # Logging level
  v_log_level: DEBUG  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # Databricks CLI configuration (optional for local environments)
  v_databricks_cli_path: "databricks"  # CLI executable path
  v_databricks_config_profile: "TEST"  # Profile name
  
  # Databricks bundle file path
  v_databricks_yml_path: "/path/to/config/databricks.yml"

# Global settings (optional)
global_settings:
  export_libraries: true  # Global control for library artifact export

# Define workflows to export
workflows:
  - job_name: "My-Workflow"
    job_id: 123456789
    is_existing: true
    is_active: true
    export_libraries: true  # Override global setting for this workflow
    
# Define pipelines to export (optional)
pipelines:
  - pipeline_name: "My-Pipeline"
    pipeline_id: "pipeline_123456"
    is_existing: true
    is_active: true
    export_libraries: false  # Override global setting for this pipeline

# Value replacements in YAML files
value_replacements:
  "old_value": "new_value"
  
# Path transformations
path_replacement:
  "^/Workspace/Repos/[^/]+/": "../"
  "^/Repos/[^/]+/": "../"
```

### New Configuration Features

#### Library Export Control
- **Global Setting**: `global_settings.export_libraries` controls library export for all assets
- **Individual Override**: Each workflow/pipeline can override the global setting
- **Hierarchy**: Global `false` overrides individual `true` settings (safety first)

#### Dual Asset Processing
- **Workflows**: Traditional Databricks job definitions
- **Pipelines**: DLT (Delta Live Tables) pipeline definitions  
- **Simultaneous Processing**: Both asset types can be processed in a single run
- **Independent Control**: Each asset type can be enabled/disabled separately

### Configuration Options Explained

| Variable | Description | Example |
|----------|-------------|---------|
| `v_start_path` | Base directory for all generated files | `/path/to/exports` |
| `v_log_directory_path` | Directory for log files | `{v_start_path}/logs` |
| `v_log_level` | Logging verbosity level | `DEBUG`, `INFO`, `WARNING` |
| `v_databricks_cli_path` | Path to Databricks CLI (local only) | `databricks` |
| `v_databricks_config_profile` | Databricks profile name (local only) | `TEST`, `PROD` |
| `v_databricks_yml_path` | Path to databricks.yml bundle file | `/path/to/databricks.yml` |

### databricks.yml Structure

```yaml
bundle:
  name: WF_EXPORTER_BUNDLE

include:
  - environments/*.yml
  - resources/*.yml

targets:
  dev:
    mode: development
    default: true
    workspace:
      # host: $DATABRICKS_HOST
```

## üìÅ Project Structure

```
wf-exporter/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ config.yml          # Main configuration
‚îÇ   ‚îî‚îÄ‚îÄ databricks.yml      # Databricks bundle config
‚îú‚îÄ‚îÄ src/wfExporter/         # Main package
‚îÇ   ‚îú‚îÄ‚îÄ cli/                # CLI management
‚îÇ   ‚îú‚îÄ‚îÄ config/             # Configuration handling
‚îÇ   ‚îú‚îÄ‚îÄ core/               # Main business logic
‚îÇ   ‚îú‚îÄ‚îÄ logging/            # Logging utilities
‚îÇ   ‚îú‚îÄ‚îÄ processing/         # File and YAML processing
‚îÇ   ‚îî‚îÄ‚îÄ workflow/           # Workflow extraction
‚îú‚îÄ‚îÄ dev_test.py             # Development testing script
‚îî‚îÄ‚îÄ pyproject.toml          # Package configuration
```

For detailed module documentation, see [MODULES.md](MODULES.md).

## üîß Development

### Development Testing

```bash
# Test your changes
poetry run python dev_test.py
```

### Building from Source

```bash
poetry build
# Output: dist/wfexporter-1.0.6-py3-none-any.whl
```

## ü§ù Authentication Options

1. **Config Profile** (Recommended for local development)
   ```yaml
   v_databricks_config_profile: "your-profile"
   ```

2. **Environment Variables**
   ```bash
   export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
   export DATABRICKS_TOKEN="your-token"
   ```

3. **Command Line Arguments**
   ```bash
   wf-export --host <host> --token <token>
   ```

4. **Auto-detection** (Databricks notebooks - no setup required)

## üìù Output Structure

After running, your configured directory will contain:

```
exports/
‚îú‚îÄ‚îÄ src/                    # Generated source files
‚îú‚îÄ‚îÄ resources/              # Generated YAML files
‚îú‚îÄ‚îÄ backup_jobs_yaml/       # Backup of original YAML
‚îú‚îÄ‚îÄ logs/                   # Log files
‚îî‚îÄ‚îÄ bind_scripts/           # Resource mapping files
```

## üêõ Troubleshooting

- **Authentication Issues**: Verify your Databricks credentials and profile configuration
- **File Permissions**: Ensure write access to output directories
- **Missing Config**: Check that both `config.yml` and `databricks.yml` exist and are properly formatted
- **CLI Not Found**: For local environments, ensure Databricks CLI is installed and accessible

---

**Version**: 1.0.6 | **Python**: 3.11+ | **License**: MIT
